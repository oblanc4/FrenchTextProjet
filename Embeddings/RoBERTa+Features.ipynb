{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>lexical_complexite</th>\n",
       "      <th>note_orthographe</th>\n",
       "      <th>char_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>complexite_texte</th>\n",
       "      <th>...</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>SCONJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.194007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160077</td>\n",
       "      <td>0.140152</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.140152</td>\n",
       "      <td>0.339713</td>\n",
       "      <td>0.244565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088078</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.039541</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.195804</td>\n",
       "      <td>0.081522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022959</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.184993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130740</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  difficulty  \\\n",
       "id                                                                  \n",
       "0   Les coûts kilométriques réels peuvent diverger...           4   \n",
       "1   Le bleu, c'est ma couleur préférée mais je n'a...           0   \n",
       "2   Le test de niveau en français est sur le site ...           0   \n",
       "3            Est-ce que ton mari est aussi de Boston?           0   \n",
       "4   Dans les écoles de commerce, dans les couloirs...           2   \n",
       "\n",
       "    lexical_complexite  note_orthographe  char_length  word_length  \\\n",
       "id                                                                   \n",
       "0             0.194007          1.000000     0.160077     0.140152   \n",
       "1             0.082334          1.000000     0.036990     0.041667   \n",
       "2             0.088078          0.769231     0.039541     0.045455   \n",
       "3             0.062664          1.000000     0.022959     0.026515   \n",
       "4             0.184993          1.000000     0.130740     0.125000   \n",
       "\n",
       "    type_token_ratio  sentence_length  avg_word_length  complexite_texte  ...  \\\n",
       "id                                                                        ...   \n",
       "0           0.467105         0.140152         0.339713          0.244565  ...   \n",
       "1           1.000000         0.041667         0.204545          0.086957  ...   \n",
       "2           0.826923         0.045455         0.195804          0.081522  ...   \n",
       "3           1.000000         0.026515         0.193182          0.054348  ...   \n",
       "4           0.602941         0.125000         0.288770          0.228261  ...   \n",
       "\n",
       "         DET      PRON       NUM      NOUN  INTJ       ADP       ADJ  \\\n",
       "id                                                                     \n",
       "0   0.066667  0.000000  0.000000  0.311111   0.0  0.288889  0.066667   \n",
       "1   0.187500  0.125000  0.000000  0.125000   0.0  0.000000  0.000000   \n",
       "2   0.200000  0.000000  0.000000  0.400000   0.0  0.266667  0.000000   \n",
       "3   0.000000  0.100000  0.000000  0.300000   0.0  0.100000  0.000000   \n",
       "4   0.095238  0.047619  0.047619  0.238095   0.0  0.261905  0.047619   \n",
       "\n",
       "        VERB   PROPN  SCONJ  \n",
       "id                           \n",
       "0   0.088889  0.0000    0.0  \n",
       "1   0.125000  0.0625    0.0  \n",
       "2   0.066667  0.0000    0.0  \n",
       "3   0.100000  0.1000    0.1  \n",
       "4   0.095238  0.0000    0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read the CSV file\n",
    "training_data = pd.read_csv(\"data/training_dataPhil.csv\", index_col=0)\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle pré-entraîné RoBERTa en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing CamembertModel.\n",
      "\n",
      "All the weights of CamembertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Utilisez un modèle pré-entraîné en français\n",
    "model_name = \"jplu/tf-camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, from_tf=True)  # Spécifiez from_tf=True ici\n",
    "\n",
    "phrases = training_data['sentence'].tolist()\n",
    "\n",
    "# Utilisez le tokenizer pour convertir les phrases en tokens BERT\n",
    "tokenized_phrases = tokenizer(phrases, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# Passez les tokens encodés au modèle BERT pour obtenir les embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**tokenized_phrases)\n",
    "\n",
    "bert_embeddings = embeddings.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVEC PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Supposons que bert_embeddings contient les embeddings BERT sous forme de tableau tridimensionnel\n",
    "# (nombre d'exemples x longueur de phrase x dimension de l'embedding)\n",
    "\n",
    "# Aplatissez les embeddings pour les rendre bidimensionnels\n",
    "num_examples, seq_length, embedding_dim = bert_embeddings.shape\n",
    "bert_embeddings_2d = bert_embeddings.reshape(num_examples, -1)  # -1 signifie que numpy doit calculer automatiquement la dimension\n",
    "\n",
    "# Appliquez PCA aux embeddings bidimensionnels\n",
    "num_components = 50  # Choisissez le nombre de composantes principales souhaité\n",
    "pca = PCA(n_components=num_components)\n",
    "reduced_embeddings = pca.fit_transform(bert_embeddings_2d)\n",
    "del pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Supposons que bert_embeddings contient les embeddings BERT sous forme de tableau tridimensionnel\n",
    "# (nombre d'exemples x longueur de phrase x dimension de l'embedding)\n",
    "\n",
    "# Aplatissez les embeddings pour les rendre bidimensionnels\n",
    "num_examples, seq_length, embedding_dim = bert_embeddings.shape\n",
    "bert_embeddings_2d = bert_embeddings.reshape(num_examples, -1)  # -1 signifie que numpy doit calculer automatiquement la dimension\n",
    "\n",
    "# Appliquez t-SNE aux embeddings bidimensionnels sans spécifier n_components\n",
    "tsne = TSNE()\n",
    "reduced_embeddings = tsne.fit_transform(bert_embeddings_2d)\n",
    "del tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "# Supposons que bert_embeddings contient les embeddings BERT sous forme de tableau tridimensionnel\n",
    "# (nombre d'exemples x longueur de phrase x dimension de l'embedding)\n",
    "\n",
    "# Aplatissez les embeddings pour les rendre bidimensionnels\n",
    "num_examples, seq_length, embedding_dim = bert_embeddings.shape\n",
    "bert_embeddings_2d = bert_embeddings.reshape(num_examples, -1)  # -1 signifie que numpy doit calculer automatiquement la dimension\n",
    "\n",
    "# Appliquez UMAP aux embeddings bidimensionnels avec le nombre de composantes souhaité\n",
    "num_components = 2  # Choisissez le nombre de composantes principales souhaité (2 pour réduire en 2D)\n",
    "reducer = umap.UMAP(n_components=num_components)\n",
    "reduced_embeddings = reducer.fit_transform(bert_embeddings_2d)\n",
    "\n",
    "del reducer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divison des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "training_data_x = training_data.drop(columns=[\"difficulty\", \"sentence\"])\n",
    "X_combined = np.concatenate((reduced_embeddings, training_data_x), axis=1)\n",
    "\n",
    "X = X_combined  # Les embeddings réduits après PCA\n",
    "y = training_data[\"difficulty\"]  # Les niveaux de difficulté (étiquettes)\n",
    "\n",
    "# Effectuez la division en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression Logistique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle de régression logistique : 0.4166666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Créez un modèle de régression logistique\n",
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluez la performance du modèle sur l'ensemble de test\n",
    "accuracy = logistic_regression_model.score(X_test, y_test)\n",
    "print(\"Précision du modèle de régression logistique :\", accuracy)\n",
    "\n",
    "del logistic_regression_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (Support Vector Machine) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle SVM : 0.3625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Créez un modèle SVM (classification)\n",
    "svm_model = SVC()\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluez la performance du modèle sur l'ensemble de test\n",
    "accuracy = svm_model.score(X_test, y_test)\n",
    "print(\"Précision du modèle SVM :\", accuracy)\n",
    "\n",
    "del svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbres de Décision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle d'arbre de décision : 0.34270833333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Créez un modèle d'arbre de décision\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluez la performance du modèle sur l'ensemble de test\n",
    "accuracy = decision_tree_model.score(X_test, y_test)\n",
    "print(\"Précision du modèle d'arbre de décision :\", accuracy)\n",
    "\n",
    "del decision_tree_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forêts Aléatoires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle de forêt aléatoire : 0.4322916666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Créez un modèle de forêt aléatoire\n",
    "random_forest_model = RandomForestClassifier()\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluez la performance du modèle sur l'ensemble de test\n",
    "accuracy = random_forest_model.score(X_test, y_test)\n",
    "print(\"Précision du modèle de forêt aléatoire :\", accuracy)\n",
    "\n",
    "del random_forest_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseaux de Neurones (utilisant scikit-learn) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle de réseau de neurones : 0.42916666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Créez un modèle de réseau de neurones (multilayer perceptron)\n",
    "mlp_model = MLPClassifier()\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluez la performance du modèle sur l'ensemble de test\n",
    "accuracy = mlp_model.score(X_test, y_test)\n",
    "print(\"Précision du modèle de réseau de neurones :\", accuracy)\n",
    "\n",
    "del mlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseaux de Neurones Récursifs - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.61      0.61       166\n",
      "           1       0.40      0.40      0.40       158\n",
      "           2       0.36      0.43      0.39       166\n",
      "           3       0.37      0.36      0.36       153\n",
      "           4       0.38      0.57      0.46       152\n",
      "           5       0.50      0.19      0.27       165\n",
      "\n",
      "    accuracy                           0.43       960\n",
      "   macro avg       0.44      0.43      0.42       960\n",
      "weighted avg       0.44      0.43      0.42       960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Créez votre modèle de classification (par exemple, Réseaux de Neurones Récursifs - RNN)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "rnn_model = MLPClassifier(hidden_layer_sizes=(100,), activation='tanh', solver='adam', max_iter=1000, random_state=42)\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "rnn_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédisez les niveaux de difficulté sur l'ensemble de test\n",
    "y_pred_rnn = rnn_model.predict(X_test)\n",
    "\n",
    "# Évaluez la performance du modèle\n",
    "accuracy_rnn = rnn_model.score(X_test, y_pred_rnn)\n",
    "print(classification_report(y_test, y_pred_rnn))\n",
    "\n",
    "del rnn_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
