{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an effort to enhance the accuracy of our model, we will augment the training set by incorporating additional attributes, such as the word count of each sentence or its complexity. This expanded feature set aims to provide the model with richer information, ultimately contributing to improved performance in our text classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/phil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/phil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.5.0/fr_core_news_sm-3.5.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from fr-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/phil/anaconda3/envs/ML/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from spellchecker import SpellChecker\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nous dûmes nous excuser des propos que nous eû...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ce n'est pas étonnant, car c'est une saison my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le corps de Golo lui-même, d'une essence aussi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence\n",
       "id                                                   \n",
       "0   Nous dûmes nous excuser des propos que nous eû...\n",
       "1   Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
       "2   Et, paradoxalement, boire froid n'est pas la b...\n",
       "3   Ce n'est pas étonnant, car c'est une saison my...\n",
       "4   Le corps de Golo lui-même, d'une essence aussi..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the file path\n",
    "file_path_training = \"../Dataset/training_data.csv\"\n",
    "file_path_test = \"../Dataset/unlabelled_test_data.csv\"\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "training_data = pd.read_csv(file_path_training, index_col=0)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPELL CHECKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "# Initialisation des outils\n",
    "tool = language_tool_python.LanguageTool('fr')\n",
    "\n",
    "def evaluer_orthographe_syntaxe(texte):\n",
    "    # Vérification avec LanguageTool\n",
    "    erreurs_language_tool = tool.check(texte)\n",
    "\n",
    "    # Compter les différents types d'erreurs\n",
    "    erreurs_orthographe = sum(1 for erreur in erreurs_language_tool if 'ORTHOGRAPH' in erreur.ruleId)\n",
    "    erreurs_grammaire = sum(1 for erreur in erreurs_language_tool if 'GRAMMAR' in erreur.ruleId)\n",
    "\n",
    "    # Analyse syntaxique avec spaCy\n",
    "    doc = nlp(texte)\n",
    "    erreurs_syntaxe = sum(1 for token in doc if token.dep_ == \"nsubj\" and token.head.pos_ != 'VERB')\n",
    "\n",
    "    # Calcul de la note\n",
    "    seuil_minimal_mots = 5\n",
    "    nombre_mots = max(len(texte.split()), seuil_minimal_mots)\n",
    "    poids_orthographe = 1.0  # Ajuster selon l'importance relative\n",
    "    poids_grammaire = 1.5  # Les erreurs grammaticales peuvent être plus graves\n",
    "    note_globale = max(1 - ((erreurs_orthographe * poids_orthographe + erreurs_grammaire * poids_grammaire + erreurs_syntaxe) / nombre_mots), 0)\n",
    "\n",
    "    return note_globale\n",
    "\n",
    "# Exemple d'utilisation\n",
    "training_data['note_orthographe'] = training_data['sentence'].apply(evaluer_orthographe_syntaxe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diversite_lexicale_complexite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines a set of functions to preprocess and analyze text data's lexical diversity and complexity. The preprocess_text function tokenizes a given text, converts words to lowercase. The diversite_lexicale_complexite function calculates the lexical diversity and complexity of a text, considering factors like the average length of words and phrases. \n",
    "\n",
    "The resulting complexity values are stored in a new column named 'lexical_complexite.' Overall, this code aids in extracting linguistic features and assessing the linguistic richness and complexity of French text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stopwords_french = set(stopwords.words('french'))\n",
    "\n",
    "def preprocess_text(texte, remove_stopwords=True):\n",
    "    if not isinstance(texte, str):\n",
    "        raise ValueError(\"Le texte doit être une chaîne de caractères.\")\n",
    "\n",
    "    mots = word_tokenize(texte, language='french')\n",
    "    mots_low = [mot.lower() for mot in mots if mot.isalpha()] \n",
    "\n",
    "    if remove_stopwords:\n",
    "        mots_low = [mot for mot in mots_low if mot not in stopwords_french]\n",
    "\n",
    "    return mots_low\n",
    "\n",
    "def diversite_lexicale_complexite(texte, remove_stopwords=True):\n",
    "    phrases = sent_tokenize(texte, language='french')\n",
    "    mots_low = preprocess_text(texte, remove_stopwords)\n",
    "    \n",
    "    if not mots_low or not phrases:\n",
    "        return float(0) \n",
    "    \n",
    "    nb_mots = len(mots_low)\n",
    "    nb_phrases = len(phrases)\n",
    "    longueur_moyenne_mot = sum(len(mot) for mot in mots_low) / nb_mots\n",
    "    longueur_moyenne_phrase = sum(len(phrase.split()) for phrase in phrases) / nb_phrases\n",
    "\n",
    "    lexical_diversity = len(set(mots_low)) / nb_mots\n",
    "\n",
    "    # Complexity factor based on average word and sentence length\n",
    "    complexite = lexical_diversity * (1 + (longueur_moyenne_mot / 5)) * (1 + (longueur_moyenne_phrase / 10))\n",
    "    \n",
    "    return complexite\n",
    "\n",
    "training_data['lexical_complexite'] = training_data['sentence'].apply(diversite_lexicale_complexite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>note_orthographe</th>\n",
       "      <th>lexical_complexite</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nous dûmes nous excuser des propos que nous eû...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>4.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ce n'est pas étonnant, car c'est une saison my...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>4.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le corps de Golo lui-même, d'une essence aussi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.204000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  note_orthographe  \\\n",
       "id                                                                        \n",
       "0   Nous dûmes nous excuser des propos que nous eû...          0.900000   \n",
       "1   Vous ne pouvez pas savoir le plaisir que j'ai ...          1.000000   \n",
       "2   Et, paradoxalement, boire froid n'est pas la b...          1.000000   \n",
       "3   Ce n'est pas étonnant, car c'est une saison my...          0.888889   \n",
       "4   Le corps de Golo lui-même, d'une essence aussi...          1.000000   \n",
       "\n",
       "    lexical_complexite  \n",
       "id                      \n",
       "0             4.700000  \n",
       "1             5.485714  \n",
       "2             4.560000  \n",
       "3             4.560000  \n",
       "4            18.204000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OTHER ATTRIBUTES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code offers a set of functions to extract linguistic features, including sentence and word metrics, lexical complexity, and POS tagging distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE LENGTH\n",
    "def sentence_length(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "# WORD LENGTH\n",
    "def average_word_length(sentence):\n",
    "    words = sentence.split()\n",
    "    return np.mean([len(word) for word in words]) if words else 0\n",
    "\n",
    "def type_token_ratio(sentence):\n",
    "    words = sentence.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "# COMPLEXITE LEXICALE\n",
    "def complexite_texte(texte):\n",
    "    doc = nlp(texte)\n",
    "\n",
    "    # Syntactic measurements\n",
    "    nb_phrases = len(list(doc.sents))\n",
    "    profondeur_moyenne = sum(len(list(phrase.root.subtree)) for phrase in doc.sents) / nb_phrases if nb_phrases > 0 else 0\n",
    "\n",
    "    # Grammatical measures\n",
    "    temps_verbaux = {mot.tag_: 0 for mot in doc if mot.tag_ and \"VERB\" in mot.tag_}\n",
    "    for mot in doc:\n",
    "        if mot.tag_ and \"VERB\" in mot.tag_:\n",
    "            temps_verbaux[mot.tag_] += 1\n",
    "    diversite_temps_verbaux = len(temps_verbaux)\n",
    "\n",
    "    complexite = profondeur_moyenne + diversite_temps_verbaux\n",
    "\n",
    "    return complexite\n",
    "\n",
    "# POS TAGGING\n",
    "def pos_tag_distribution(sentence):\n",
    "    if not isinstance(sentence, str):\n",
    "        raise ValueError(\"L'entrée doit être une chaîne de caractères.\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    pos_counts = {pos: 0 for pos in [token.pos_ for token in doc]}  \n",
    "\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        pos_counts[pos] += 1\n",
    "\n",
    "    # Optional: normalize by total number of words\n",
    "    total_mots = len(doc)\n",
    "    if total_mots > 0:\n",
    "        pos_counts_normalized = {pos: count / total_mots for pos, count in pos_counts.items()}\n",
    "        return pos_counts_normalized\n",
    "\n",
    "    return pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['char_length'] = training_data['sentence'].apply(len)\n",
    "training_data['word_length'] = training_data['sentence'].apply(lambda x: len(x.split()))\n",
    "training_data['type_token_ratio'] = training_data['sentence'].apply(type_token_ratio)\n",
    "\n",
    "training_data['sentence_length'] = training_data['sentence'].apply(sentence_length)\n",
    "training_data['avg_word_length'] = training_data['sentence'].apply(average_word_length)\n",
    "training_data['complexite_texte'] = training_data['sentence'].apply(complexite_texte)\n",
    "training_data['pos_tags'] = training_data['sentence'].apply(pos_tag_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.4' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.3' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.125' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.0625' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.07692307692307693' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.012048192771084338' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.03614457831325301' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.18181818181818182' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n",
      "/var/folders/wm/59w25n_j2xg5z1dlk3fzxnl00000gn/T/ipykernel_6953/1960565483.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.00847457627118644' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  training_data.at[index, tag] = count\n"
     ]
    }
   ],
   "source": [
    "unique_pos_tags = set()\n",
    "for pos_tags_dict in training_data['pos_tags']:\n",
    "    unique_pos_tags.update(pos_tags_dict.keys())\n",
    "\n",
    "# Initialize columns for each POS tag with default value 0\n",
    "for tag in ['PUNCT', 'ADV', 'CCONJ', 'X', 'AUX', 'DET', 'PRON', 'NUM', 'NOUN', 'INTJ', 'ADP', 'ADJ', 'VERB', 'PROPN', 'SCONJ']:\n",
    "    training_data[tag] = 0\n",
    "\n",
    "# Populate the columns with counts\n",
    "for index, row in training_data.iterrows():\n",
    "    for tag, count in row['pos_tags'].items():\n",
    "        if tag in training_data.columns:\n",
    "            training_data.at[index, tag] = count\n",
    "\n",
    "training_data = training_data.drop(['pos_tags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>note_orthographe</th>\n",
       "      <th>lexical_complexite</th>\n",
       "      <th>char_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>complexite_texte</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>...</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>SCONJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nous dûmes nous excuser des propos que nous eû...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.485714</td>\n",
       "      <td>79</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ce n'est pas étonnant, car c'est une saison my...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le corps de Golo lui-même, d'une essence aussi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.204000</td>\n",
       "      <td>460</td>\n",
       "      <td>72</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>72</td>\n",
       "      <td>5.402778</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.036145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  note_orthographe  \\\n",
       "id                                                                        \n",
       "0   Nous dûmes nous excuser des propos que nous eû...          0.900000   \n",
       "1   Vous ne pouvez pas savoir le plaisir que j'ai ...          1.000000   \n",
       "2   Et, paradoxalement, boire froid n'est pas la b...          1.000000   \n",
       "3   Ce n'est pas étonnant, car c'est une saison my...          0.888889   \n",
       "4   Le corps de Golo lui-même, d'une essence aussi...          1.000000   \n",
       "\n",
       "    lexical_complexite  char_length  word_length  type_token_ratio  \\\n",
       "id                                                                   \n",
       "0             4.700000           59           10          0.900000   \n",
       "1             5.485714           79           14          1.000000   \n",
       "2             4.560000           58            9          1.000000   \n",
       "3             4.560000           55            9          1.000000   \n",
       "4            18.204000          460           72          0.791667   \n",
       "\n",
       "    sentence_length  avg_word_length  complexite_texte     PUNCT  ...  \\\n",
       "id                                                                ...   \n",
       "0                10         5.000000              11.0  0.000000  ...   \n",
       "1                14         4.714286              17.0  0.062500  ...   \n",
       "2                 9         5.555556              14.0  0.230769  ...   \n",
       "3                 9         5.222222              12.0  0.083333  ...   \n",
       "4                72         5.402778              84.0  0.072289  ...   \n",
       "\n",
       "         DET      PRON  NUM      NOUN  INTJ       ADP       ADJ      VERB  \\\n",
       "id                                                                          \n",
       "0   0.000000  0.400000  0.0  0.100000     0  0.100000  0.100000  0.300000   \n",
       "1   0.125000  0.187500  0.0  0.062500     0  0.062500  0.125000  0.250000   \n",
       "2   0.076923  0.000000  0.0  0.076923     0  0.000000  0.153846  0.076923   \n",
       "3   0.083333  0.166667  0.0  0.083333     0  0.000000  0.166667  0.000000   \n",
       "4   0.156627  0.120482  0.0  0.156627     0  0.120482  0.084337  0.096386   \n",
       "\n",
       "       PROPN     SCONJ  \n",
       "id                      \n",
       "0   0.000000  0.000000  \n",
       "1   0.000000  0.000000  \n",
       "2   0.000000  0.000000  \n",
       "3   0.000000  0.000000  \n",
       "4   0.012048  0.036145  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if 'difficulty' in training_data.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    training_data['difficulty'] = label_encoder.fit_transform(training_data['difficulty'])\n",
    "\n",
    "numerical_features = ['lexical_complexite', 'note_orthographe', 'char_length', 'word_length', 'type_token_ratio', 'sentence_length', 'avg_word_length', 'complexite_texte']\n",
    "\n",
    "# scaler MinMax\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "training_data[numerical_features] = scaler.fit_transform(training_data[numerical_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save File --> csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv('training_dataUP.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
