{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path_test = \"unlabelled_test_data.csv\"\n",
    "file_path_training_processed = \"training_data_processed.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "unlabelled_test_data = pd.read_csv(file_path_test)\n",
    "training_data = pd.read_csv(file_path_training_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(training_data['processed_sentence'])\n",
    "y = training_data['difficulty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "240/240 [==============================] - 2s 5ms/step - loss: 1.5963 - accuracy: 0.3221 - val_loss: 1.3357 - val_accuracy: 0.4083\n",
      "Epoch 2/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.0206 - accuracy: 0.6307 - val_loss: 1.2556 - val_accuracy: 0.4708\n",
      "Epoch 3/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.5050 - accuracy: 0.8432 - val_loss: 1.4420 - val_accuracy: 0.4531\n",
      "Epoch 4/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.9281 - val_loss: 1.6770 - val_accuracy: 0.4573\n",
      "Epoch 5/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.9609 - val_loss: 1.8917 - val_accuracy: 0.4573\n",
      "Epoch 6/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0938 - accuracy: 0.9763 - val_loss: 2.0995 - val_accuracy: 0.4469\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.0995 - accuracy: 0.4469\n",
      "Accuracy: 0.4468750059604645\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming the training data is loaded into a DataFrame named 'training_data'\n",
    "# with columns 'sentence' and 'difficulty'\n",
    "\n",
    "# Label encoding the 'difficulty' column\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(training_data['difficulty'])\n",
    "\n",
    "# Converting the encoded labels to categorical format\n",
    "y_categorical = to_categorical(encoded_labels)\n",
    "\n",
    "# Using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(training_data['processed_sentence']).toarray()\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.20, random_state=42)\n",
    "\n",
    "# Building the Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compiling the Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the Model\n",
    "history = model.fit(X_train, y_train, epochs=6, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/oanaalexandrablanc/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(training_data['processed_sentence']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "additional_features = scaler.fit_transform(training_data[['sentence_length', 'avg_word_length', 'type_token_ratio', 'syntactic_complexity', 'PUNCT', 'ADV', 'CCONJ', 'X', 'AUX', 'DET', 'PRON', 'NUM', 'NOUN', 'INTJ', 'ADP', 'ADJ', 'VERB', 'PROPN', 'SCONJ']])\n",
    "\n",
    "# Standardize these features\n",
    "additional_features_scaled = scaler.fit_transform(additional_features)\n",
    "\n",
    "# Combine with TF-IDF features\n",
    "X_combined = np.hstack([X_tfidf, additional_features_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 1.4037 - accuracy: 0.3964\n",
      "Epoch 2/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 1.0772 - accuracy: 0.5669\n",
      "Epoch 3/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.7057 - accuracy: 0.7578\n",
      "Epoch 4/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.3516 - accuracy: 0.8971\n",
      "Epoch 5/6\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.1797 - accuracy: 0.9523\n",
      "Epoch 6/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1034 - accuracy: 0.9745\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.6980 - accuracy: 0.4635\n",
      "Epoch 1/6\n",
      "240/240 [==============================] - 2s 3ms/step - loss: 1.4079 - accuracy: 0.3839\n",
      "Epoch 2/6\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.0501 - accuracy: 0.5792\n",
      "Epoch 3/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.7820\n",
      "Epoch 4/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.3514 - accuracy: 0.8943\n",
      "Epoch 5/6\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.1916 - accuracy: 0.9495\n",
      "Epoch 6/6\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.1171 - accuracy: 0.9719\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.6856 - accuracy: 0.4938\n",
      "Epoch 1/6\n",
      "240/240 [==============================] - 2s 4ms/step - loss: 1.3971 - accuracy: 0.3917\n",
      "Epoch 2/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.0618 - accuracy: 0.5753\n",
      "Epoch 3/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.7737\n",
      "Epoch 4/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.3513 - accuracy: 0.8995\n",
      "Epoch 5/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1772 - accuracy: 0.9513\n",
      "Epoch 6/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1101 - accuracy: 0.9719\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.6960 - accuracy: 0.5042\n",
      "Epoch 1/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 1.3935 - accuracy: 0.3927\n",
      "Epoch 2/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 1.0457 - accuracy: 0.5865\n",
      "Epoch 3/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.6263 - accuracy: 0.8016\n",
      "Epoch 4/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.3072 - accuracy: 0.9117\n",
      "Epoch 5/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.1621 - accuracy: 0.9581\n",
      "Epoch 6/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.1131 - accuracy: 0.9695\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.7600 - accuracy: 0.4823\n",
      "Epoch 1/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 1.4120 - accuracy: 0.3849\n",
      "Epoch 2/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 1.0714 - accuracy: 0.5758\n",
      "Epoch 3/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.6805 - accuracy: 0.7784\n",
      "Epoch 4/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.3466 - accuracy: 0.8971\n",
      "Epoch 5/6\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1967 - accuracy: 0.9482\n",
      "Epoch 6/6\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.1187 - accuracy: 0.9698\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.5741 - accuracy: 0.5323\n",
      "Average accuracy: 0.495208328962326, Standard Deviation: 0.022922344772280924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Number of folds\n",
    "num_folds = 5\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Assuming 'X_combined' is your feature matrix and 'encoded_labels' are your labels\n",
    "accuracies = []\n",
    "\n",
    "for train, test in kfold.split(X_combined, encoded_labels):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X_combined[train], X_combined[test]\n",
    "    y_train, y_test = encoded_labels[train], encoded_labels[test]\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    y_train_categorical = to_categorical(y_train)\n",
    "    y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(y_train_categorical.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_categorical, epochs=6, batch_size=16)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test_categorical)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate the average and standard deviation of the accuracies\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(f\"Average accuracy: {avg_accuracy}, Standard Deviation: {std_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
