{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code performs **fine-tuning** on a pre-trained CamemBERT model for classifying the difficulty levels of sentences. It utilizes a training dataset containing sentences and their associated difficulty labels. The sentences are tokenized, and the tokens are used to create a training dataset. The model is then trained over multiple epochs by minimizing the CrossEntropy loss between the model predictions and the actual difficulty levels. The Adam optimizer is employed to adjust the model weights. \n",
    "\n",
    "**This process adapts the pre-trained model to the specific task of classifying sentence difficulty levels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use fine-tuning with the complete dataset (without augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>note_orthographe</th>\n",
       "      <th>lexical_complexite</th>\n",
       "      <th>char_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>complexite_texte</th>\n",
       "      <th>...</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>SCONJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194007</td>\n",
       "      <td>0.160077</td>\n",
       "      <td>0.140152</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.140152</td>\n",
       "      <td>0.339713</td>\n",
       "      <td>0.244565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082334</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.088078</td>\n",
       "      <td>0.039541</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.195804</td>\n",
       "      <td>0.081522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062664</td>\n",
       "      <td>0.022959</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.184993</td>\n",
       "      <td>0.130740</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  difficulty  \\\n",
       "id                                                                  \n",
       "0   Les coûts kilométriques réels peuvent diverger...           4   \n",
       "1   Le bleu, c'est ma couleur préférée mais je n'a...           0   \n",
       "2   Le test de niveau en français est sur le site ...           0   \n",
       "3            Est-ce que ton mari est aussi de Boston?           0   \n",
       "4   Dans les écoles de commerce, dans les couloirs...           2   \n",
       "\n",
       "    note_orthographe  lexical_complexite  char_length  word_length  \\\n",
       "id                                                                   \n",
       "0           1.000000            0.194007     0.160077     0.140152   \n",
       "1           1.000000            0.082334     0.036990     0.041667   \n",
       "2           0.769231            0.088078     0.039541     0.045455   \n",
       "3           1.000000            0.062664     0.022959     0.026515   \n",
       "4           1.000000            0.184993     0.130740     0.125000   \n",
       "\n",
       "    type_token_ratio  sentence_length  avg_word_length  complexite_texte  ...  \\\n",
       "id                                                                        ...   \n",
       "0           0.467105         0.140152         0.339713          0.244565  ...   \n",
       "1           1.000000         0.041667         0.204545          0.086957  ...   \n",
       "2           0.826923         0.045455         0.195804          0.081522  ...   \n",
       "3           1.000000         0.026515         0.193182          0.054348  ...   \n",
       "4           0.602941         0.125000         0.288770          0.228261  ...   \n",
       "\n",
       "         DET      PRON       NUM      NOUN  INTJ       ADP       ADJ  \\\n",
       "id                                                                     \n",
       "0   0.066667  0.000000  0.000000  0.311111   0.0  0.288889  0.066667   \n",
       "1   0.187500  0.125000  0.000000  0.125000   0.0  0.000000  0.000000   \n",
       "2   0.200000  0.000000  0.000000  0.400000   0.0  0.266667  0.000000   \n",
       "3   0.000000  0.100000  0.000000  0.300000   0.0  0.100000  0.000000   \n",
       "4   0.095238  0.047619  0.047619  0.238095   0.0  0.261905  0.047619   \n",
       "\n",
       "        VERB   PROPN  SCONJ  \n",
       "id                           \n",
       "0   0.088889  0.0000    0.0  \n",
       "1   0.125000  0.0625    0.0  \n",
       "2   0.066667  0.0000    0.0  \n",
       "3   0.100000  0.1000    0.1  \n",
       "4   0.095238  0.0000    0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "training_data = pd.read_csv(\"../Dataset_upgrade/training_dataUP.csv\", index_col=0)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CamemBERT Large\n",
    "model_name = \"dangvantuan/sentence-camembert-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "sentences = training_data['sentence'].tolist()\n",
    "labels = training_data['difficulty'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at dangvantuan/sentence-camembert-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and model initialization\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "model = CamembertForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fine-tunes a CamemBERT model on a dataset for sentence difficulty classification. It creates a training dataset, defines a CrossEntropy loss function, and utilizes the Adam optimizer for model training over multiple epochs, displaying the average loss per epoch. The tqdm library is used for a progress bar during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not run, as this is very time-consuming. The generated model can be found in the file model_only.pth**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the necessary tensors from the BatchEncoding\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "\n",
    "# Create a training dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, torch.tensor(labels))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Model training\n",
    "for epoch in range(6):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Use tqdm for the progress bar\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{6}\"):\n",
    "        input_ids_batch, attention_mask_batch, labels_batch = batch\n",
    "\n",
    "        # Build the input dictionary for the model\n",
    "        inputs = {'input_ids': input_ids_batch, 'attention_mask': attention_mask_batch, 'labels': labels_batch}\n",
    "           \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Display the average loss for the epoch\n",
    "    print(f\"Average loss for epoch: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saves the state dictionary of the PyTorch model to a file named \"modele.pth\". This state dictionary contains all the learnable parameters of the model and their current values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"modele.pth\")\n",
    "# torch.save({\n",
    "#    'epoch': epoch,\n",
    "#    'model_state_dict': model.state_dict(),\n",
    "#    'optimizer_state_dict': optimizer.state_dict(),\n",
    "#    'loss': loss,  # Vous pouvez sauvegarder la perte moyenne ou la dernière perte enregistrée\n",
    "#    # Ajoutez d'autres métriques si nécessaire\n",
    "#}, \"complet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained Camembert model and its corresponding tokenizer are loaded. Subsequently, the saved state of a post-trained model is loaded from the file \"complet.pth,\" and the model is switched to evaluation mode using model.eval(). This allows the model to be used for making predictions or inferences on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the training data with data augmentation. To augment the data, we simply translated the sentences into English and then back into French. This enables us to have more data and slightly different sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"../Dataset_upgrade/augmented_training_dataUP.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at dangvantuan/sentence-camembert-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and pre-trained template\n",
    "model_name = \"dangvantuan/sentence-camembert-large\"\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"dangvantuan/sentence-camembert-large\")\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"dangvantuan/sentence-camembert-large\", num_labels=6)\n",
    "\n",
    "# Load the saved state of your post-training model\n",
    "model.load_state_dict(torch.load(\"model_only.pth\"))\n",
    "#model.load_state_dict(torch.load(\"complet.pth\")['model_state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:   0%|          | 0/9600 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing sentences: 100%|██████████| 9600/9600 [26:10<00:00,  6.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentences = training_data['sentence'].tolist()\n",
    "labels = training_data['difficulty'].tolist()\n",
    "\n",
    "# Initialize a list to store the extracted features\n",
    "all_features = []\n",
    "\n",
    "# Loop over the data with a progress bar\n",
    "for sentence in tqdm(sentences, desc=\"Processing sentences\"):\n",
    "    # Tokenization and feature extraction for each sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    features = outputs.logits.squeeze().detach().numpy()\n",
    "    \n",
    "    # Add the features to the list\n",
    "    all_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_x = training_data.drop(columns=[\"difficulty\", \"sentence\"])\n",
    "combined_features = np.concatenate((all_features, training_data_x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = combined_features  # embeddings camembert\n",
    "y = training_data[\"difficulty\"]  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.1s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.1s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.1s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.1s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.1s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.2s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.2s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.2s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.2s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.2s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.2s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.4s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.5s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.5s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.5s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.5s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.5s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.6s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.6s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.6s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.5s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.3s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.4s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.4s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.4s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.4s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.4s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.3s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   2.7s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   2.8s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   2.6s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   2.1s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   2.3s\n",
      "[CV] END ....................C=100, gamma=scale, kernel=poly; total time=   0.4s\n",
      "[CV] END ....................C=100, gamma=scale, kernel=poly; total time=   0.4s\n",
      "[CV] END ....................C=100, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END ....................C=100, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END ....................C=100, gamma=scale, kernel=poly; total time=   0.3s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.4s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.4s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.4s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.3s\n",
      "[CV] END .................C=100, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END .................C=100, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END .................C=100, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END .................C=100, gamma=scale, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .................C=100, gamma=scale, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.9s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.7s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.8s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.9s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   2.3s\n",
      "[CV] END .....................C=100, gamma=auto, kernel=poly; total time=   0.4s\n",
      "[CV] END .....................C=100, gamma=auto, kernel=poly; total time=   0.5s\n",
      "[CV] END .....................C=100, gamma=auto, kernel=poly; total time=   0.5s\n",
      "[CV] END .....................C=100, gamma=auto, kernel=poly; total time=   0.4s\n",
      "[CV] END .....................C=100, gamma=auto, kernel=poly; total time=   0.4s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.3s\n",
      "[CV] END ..................C=100, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=100, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=100, gamma=auto, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ..................C=100, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "[CV] END ..................C=100, gamma=auto, kernel=sigmoid; total time=   0.2s\n",
      "Meilleurs paramètres :  {'C': 100, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Précision avec les meilleurs paramètres :  0.9114583333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  \n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']  \n",
    "}\n",
    "\n",
    "# SVM + GridSearchCV\n",
    "svm = SVC()\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Précision avec les meilleurs paramètres : \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle SVM : 0.9135416666666667\n",
      "Accuracy of the SVM model: 0.9135416666666667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       311\n",
      "           1       0.88      0.91      0.90       327\n",
      "           2       0.84      0.88      0.86       323\n",
      "           3       0.93      0.92      0.92       344\n",
      "           4       0.93      0.92      0.93       317\n",
      "           5       0.95      0.91      0.93       298\n",
      "\n",
      "    accuracy                           0.91      1920\n",
      "   macro avg       0.92      0.91      0.91      1920\n",
      "weighted avg       0.91      0.91      0.91      1920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "#  SVM (classification)\n",
    "svm_model = SVC(C=10, gamma='scale', kernel='rbf')\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "# Save the model and tokenizer\n",
    "\n",
    "#joblib.dump(svm_model, 'svm_model.pkl')\n",
    "#tokenizer.save_pretrained(\"camembert_tokenizer\")\n",
    "#model.save_pretrained(\"camembert_model\")\n",
    "\n",
    "accuracy = svm_model.score(X_test, y_test)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(\"Précision du modèle SVM :\", accuracy)\n",
    "\n",
    "# Accuracy using accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the SVM model:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Predicting for new data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../Dataset_upgrade/unlabelled_test_dataUP.csv\", index_col=0)\n",
    "test_data2 = pd.read_csv(\"../Dataset/unlabelled_test_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nous dûmes nous excuser des propos que nous eû...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ce n'est pas étonnant, car c'est une saison my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le corps de Golo lui-même, d'une essence aussi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence\n",
       "id                                                   \n",
       "0   Nous dûmes nous excuser des propos que nous eû...\n",
       "1   Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
       "2   Et, paradoxalement, boire froid n'est pas la b...\n",
       "3   Ce n'est pas étonnant, car c'est une saison my...\n",
       "4   Le corps de Golo lui-même, d'une essence aussi..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 1200/1200 [02:30<00:00,  7.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentences = test_data['sentence'].tolist()\n",
    "\n",
    "all_features2 = []\n",
    "\n",
    "for sentence in tqdm(sentences, desc=\"Processing sentences\"):\n",
    "    # Tokenisation et extraction de caractéristiques pour chaque phrase\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    features = outputs.logits.squeeze().detach().numpy()\n",
    "    \n",
    "    all_features2.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_x = test_data.drop(columns=[\"sentence\",])\n",
    "combined_features = np.concatenate((all_features2, test_data_x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             sentence  difficulty\n",
      "id                                                               \n",
      "0   Nous dûmes nous excuser des propos que nous eû...           5\n",
      "1   Vous ne pouvez pas savoir le plaisir que j'ai ...           2\n",
      "2   Et, paradoxalement, boire froid n'est pas la b...           3\n",
      "3   Ce n'est pas étonnant, car c'est une saison my...           1\n",
      "4   Le corps de Golo lui-même, d'une essence aussi...           5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predictions\n",
    "predicted_labels = svm_model.predict(combined_features)\n",
    "test_data2['difficulty'] = predicted_labels\n",
    "\n",
    "print(test_data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2 = test_data2.drop(columns=['sentence'])\n",
    "difficulty_mapping = {\n",
    "    0: 'A1',\n",
    "    1: 'A2',\n",
    "    2: 'B1',\n",
    "    3: 'B2',\n",
    "    4: 'C1',\n",
    "    5: 'C2'\n",
    "}\n",
    "\n",
    "test_data2['difficulty'] = test_data2['difficulty'].map(difficulty_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   difficulty\n",
      "id           \n",
      "0          C2\n",
      "1          B1\n",
      "2          B2\n",
      "3          A2\n",
      "4          C2\n",
      "5          C2\n",
      "6          A2\n",
      "7          A2\n",
      "8          C1\n",
      "9          A2\n",
      "10         A2\n",
      "11         A2\n",
      "12         B2\n",
      "13         C1\n",
      "14         A1\n",
      "15         A2\n",
      "16         C1\n",
      "17         A2\n",
      "18         A2\n",
      "19         A2\n",
      "20         C2\n",
      "21         C1\n",
      "22         C1\n",
      "23         C1\n",
      "24         B1\n",
      "25         C2\n",
      "26         A1\n",
      "27         A1\n",
      "28         C2\n",
      "29         B2\n"
     ]
    }
   ],
   "source": [
    "print(test_data2.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2.to_csv('philippe+augmentation2.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
