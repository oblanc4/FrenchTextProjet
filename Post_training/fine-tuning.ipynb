{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  difficulty\n",
       "id                                                               \n",
       "0   Les coûts kilométriques réels peuvent diverger...           4\n",
       "1   Le bleu, c'est ma couleur préférée mais je n'a...           0\n",
       "2   Le test de niveau en français est sur le site ...           0\n",
       "3            Est-ce que ton mari est aussi de Boston?           0\n",
       "4   Dans les écoles de commerce, dans les couloirs...           2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "training_data = pd.read_csv(\"./Dataset_upgrade/training_dataUP.csv\", index_col=0)\n",
    "training_data = training_data[['sentence', 'difficulty']]\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"dangvantuan/sentence-camembert-large\"\n",
    "#model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = training_data['sentence'].tolist()\n",
    "labels = training_data['difficulty'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at dangvantuan/sentence-camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and model initialization\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "model = CamembertForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 1/6: 100%|██████████| 300/300 [3:13:39<00:00, 38.73s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss moyenne pour l'époque: 1.2588747866948446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 2/6: 100%|██████████| 300/300 [3:11:17<00:00, 38.26s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss moyenne pour l'époque: 0.8929875550667444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 3/6: 100%|██████████| 300/300 [3:08:05<00:00, 37.62s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss moyenne pour l'époque: 0.6508233609795571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 4/6: 100%|██████████| 300/300 [3:26:02<00:00, 41.21s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss moyenne pour l'époque: 0.4309373359878858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 5/6: 100%|██████████| 300/300 [3:09:42<00:00, 37.94s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss moyenne pour l'époque: 0.31548847556114196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 6/6: 100%|██████████| 300/300 [3:09:30<00:00, 37.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss moyenne pour l'époque: 0.18432556837797165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Recovery of tensors required for BatchEncoding\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "\n",
    "# Create a training dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, torch.tensor(labels))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Model training\n",
    "for epoch in range(6):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Époque {epoch+1}/{6}\"):\n",
    "        input_ids_batch, attention_mask_batch, labels_batch = batch\n",
    "\n",
    "        inputs = {'input_ids': input_ids_batch, 'attention_mask': attention_mask_batch, 'labels': labels_batch}\n",
    "           \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Loss moyenne pour l'époque: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"save4/modele.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Après l'entraînement de chaque époque\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,  # Vous pouvez sauvegarder la perte moyenne ou la dernière perte enregistrée\n",
    "    # Ajoutez d'autres métriques si nécessaire\n",
    "}, \"save4/complet.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTILISER LE MODEL POST-TRAIN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at dangvantuan/sentence-camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer et le modèle pré-entraîné\n",
    "model_name = \"dangvantuan/sentence-camembert-large\"\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"dangvantuan/sentence-camembert-large\")\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"dangvantuan/sentence-camembert-large\", num_labels=6)\n",
    "\n",
    "# Charger l'état sauvegardé de votre modèle post-entraîné\n",
    "model.load_state_dict(torch.load(\"model_only.pth\"))\n",
    "#model.load_state_dict(torch.load(\"complet.pth\")['model_state_dict'])\n",
    "\n",
    "# Mettre le modèle en mode évaluation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy du modèle : 0.9477083333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que sentences_test et labels_test sont vos ensembles de test\n",
    "sentences_test = training_data['sentence'].tolist()\n",
    "\n",
    "labels_test = training_data['difficulty'].tolist()\n",
    "\n",
    "# Prédiction et évaluation\n",
    "predicted_labels = []\n",
    "\n",
    "for sentence in sentences_test:\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "# Calcul de l'accuracy\n",
    "accuracy = accuracy_score(labels_test, predicted_labels)\n",
    "print(\"Accuracy du modèle :\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../Dataset_upgrade/unlabelled_test_dataUP.csv\", index_col=0)\n",
    "test_data2 = pd.read_csv(\"../Dataset/unlabelled_test_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             sentence  difficulty\n",
      "id                                                               \n",
      "0   Nous dûmes nous excuser des propos que nous eû...           5\n",
      "1   Vous ne pouvez pas savoir le plaisir que j'ai ...           2\n",
      "2   Et, paradoxalement, boire froid n'est pas la b...           2\n",
      "3   Ce n'est pas étonnant, car c'est une saison my...           1\n",
      "4   Le corps de Golo lui-même, d'une essence aussi...           5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences_test = test_data['sentence'].tolist()\n",
    "\n",
    "# Prédictions\n",
    "model.eval()  \n",
    "predicted_labels = []\n",
    "\n",
    "for sentence in sentences_test:\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "# prediction\n",
    "test_data2['difficulty'] = predicted_labels\n",
    "\n",
    "print(test_data2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2 = test_data2.drop(columns=['sentence'])\n",
    "difficulty_mapping = {\n",
    "    0: 'A1',\n",
    "    1: 'A2',\n",
    "    2: 'B1',\n",
    "    3: 'B2',\n",
    "    4: 'C1',\n",
    "    5: 'C2'\n",
    "}\n",
    "\n",
    "test_data2['difficulty'] = test_data2['difficulty'].map(difficulty_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   difficulty\n",
      "id           \n",
      "0          C2\n",
      "1          B1\n",
      "2          B1\n",
      "3          A2\n",
      "4          C2\n"
     ]
    }
   ],
   "source": [
    "print(test_data2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2.to_csv('philippe.csv', index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
