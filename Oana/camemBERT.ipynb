{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I'll try text embeddings\n",
    "#CamemBERT -> pip install transformers torch, pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the functions and training_data are properly defined\n",
    "\n",
    "# Compute additional features and add them to the DataFrame\n",
    "training_data['sentence_length'] = training_data['sentence'].apply(sentence_length)\n",
    "training_data['avg_word_length'] = training_data['sentence'].apply(average_word_length)\n",
    "training_data['type_token_ratio'] = training_data['sentence'].apply(type_token_ratio)\n",
    "training_data['syntactic_complexity'] = training_data['sentence'].apply(syntactic_complexity)\n",
    "\n",
    "# Load tokenizer and model for CamemBERT\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Tokenize and encode sentences in the dataset\n",
    "inputs = tokenizer(list(training_data['sentence']), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "batch_size = 8  # Adjust based on your system's capability\n",
    "dataset = TensorDataset(input_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate embeddings in batches\n",
    "embeddings = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[0]\n",
    "        outputs = model(input_ids)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(batch_embeddings)\n",
    "embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "\n",
    "# Combine CamemBERT embeddings with additional features\n",
    "combined_features = np.hstack((embeddings, \n",
    "                               training_data[['sentence_length', 'avg_word_length', 'type_token_ratio', 'syntactic_complexity']].values))\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, training_data['difficulty'], test_size=0.2)\n",
    "\n",
    "# Train logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CamemBERT tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Tokenize and encode sentences in the dataset\n",
    "inputs = tokenizer(list(unlabelled_test_data['sentence']), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "batch_size = 8  # Adjust based on your system's capability\n",
    "dataset = TensorDataset(input_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the CamemBERT model\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Generate embeddings in batches\n",
    "embeddings = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[0]\n",
    "        outputs = model(input_ids)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(batch_embeddings)\n",
    "embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "\n",
    "# Compute additional features for the unlabelled test data\n",
    "unlabelled_test_data['sentence_length'] = unlabelled_test_data['sentence'].apply(sentence_length)\n",
    "unlabelled_test_data['avg_word_length'] = unlabelled_test_data['sentence'].apply(average_word_length)\n",
    "unlabelled_test_data['type_token_ratio'] = unlabelled_test_data['sentence'].apply(type_token_ratio)\n",
    "unlabelled_test_data['syntactic_complexity'] = unlabelled_test_data['sentence'].apply(syntactic_complexity)\n",
    "\n",
    "# Combine CamemBERT embeddings with additional features\n",
    "combined_features = np.hstack((embeddings, \n",
    "                               unlabelled_test_data[['sentence_length', 'avg_word_length', 'type_token_ratio', 'syntactic_complexity']].values))\n",
    "\n",
    "# Make predictions using the trained logistic regression model\n",
    "predictions = logistic_model.predict(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'id': unlabelled_test_data['id'],\n",
    "    'predicted_difficulty': predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('predicted_difficulties_camembert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
